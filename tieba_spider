# /usr/bin/python
# -*- coding: utf-8  -*-

import urllib
import urllib2


def write_page(html,filename):
    '''
    主要将html内容写入到本地
    html：服务器相应的文件
    '''
    print('正在保存' + filename + '...')
    # 文件写入
    with open(filename,'w') as f:
        f.write(html)
    print('-' * 30)


def load_page(url, filename):
    '''
    作用：根据url发送请求，获取服务器响应文件
    url：需要爬取的url地址
    filename：保存的文件名
    '''

    headers = { 'User-Agent': 'Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11'}
    print('正在下载' + filename + '...')
    request = urllib2.Request(url, headers=headers)
    return urllib2.urlopen(request).read()


def tieba_spider(url, begin_page, end_page):
    '''
    作用：贴吧爬虫调度器，负责组合处理每个页面的url
    url：贴吧url的前部分
    begin_page：起始页
    end_page:结束页
    :return:
    '''
    for page in range(begin_page,end_page + 1):
        pn = str((page - 1) * 50)
        filename = kw + '第' + str(page) + '页.html'
        new_full_url = url + '&pn=' + pn
        html = load_page(new_full_url, filename)
        print(html)
        write_page(html,filename)

        # print(new_full_url)
    print('谢谢使用...'.center(30))

if __name__ == '__main__':
    kw = raw_input('请输入要爬取的贴吧名：')
    begin_page = int(raw_input('请输入起始页：'))
    end_page = int(raw_input('请输入结束页：'))

    url = 'http://tieba.baidu.com/f?'
    key = urllib.urlencode({'kw': kw})
    full_url = url + key
    tieba_spider(full_url,begin_page,end_page)





